# Changelog

All notable changes to Igris Runtime are documented here. The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/), and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

**Current Version:** 1.6.0 (Released 2025-12-28)

> **Note on Future Versions:** Versions 1.7.0-1.9.0 are marked as "Coming Soon" but many of their features are already available in v1.6.0. We document features as they're implemented, with official version bumps following after additional testing and optimization.

---

## [1.9.0] - Coming Soon Advanced Intelligence & Scale

**Note:** Federated Learning, Model Management, Human-in-Loop, and Testing/Simulation features are already implemented. Future release will include additional performance optimizations.

### Multi-Device Learning (Federated Learning)

Train your AI across multiple devices without sharing sensitive data.

**What you can do:**
- Improve AI across 50 robots without centralizing customer data
- Privacy-compliant learning for healthcare, finance, retail
- Works offline—devices learn together on local networks

**Quick start:**
```json5
{ federated: { enabled: true, min_participants: 3 } }
```

### Smart Model Selection

Let Runtime automatically pick the best model for each task.

**What you get:**
- Fast model for simple questions, powerful model for complex reasoning
- Switch models in < 200ms without restart
- Keep only your most-used models in memory

**Example:** Chat uses fast-model, code generation uses specialized code-model automatically.

### Human Oversight for Critical Decisions

Add approval workflows for AI decisions that matter.

**Use cases:**
- Industrial robots ask permission before risky moves
- Financial systems require approval for large transactions
- Medical AI escalates uncertain diagnoses

**How it works:** AI auto-approves when confident (90%+), escalates uncertain decisions to humans.

### Test Before You Deploy

Break things safely with chaos testing and virtual swarms.

**What you can test:**
- Simulate 100 robots without buying 100 robots
- Inject random failures to test resilience
- Benchmark performance under load

**Example:** Test leader election by killing the leader—verify new leader elected in < 2s.

---

## [1.8.0] - Coming Soon Robotics & Industrial AI

**Note:** ROS2 integration and Fleet Management are already available in v1.6.0. These features are production-ready now.

### Robot Control (ROS2 Integration)

Connect your AI to robots and autonomous systems.

**What you can do:**
- "Navigate to loading dock" → AI sends ROS2 navigation command
- Multi-robot coordination in warehouses
- Works with existing ROS2 infrastructure

**Quick start:**
```json5
{ ros2: { enabled: true, enable_nav2: true } }
```

### Hardware Integration (Sensors & Actuators)

Let your AI see, hear, and control hardware.

**Supported hardware:**
- Cameras (USB/CSI) for visual input
- LIDAR for environment mapping
- GPIO pins for custom sensors/actuators

**Example:** AI analyzes camera feed → Detects issue → Triggers alert via GPIO.

### Safety for Critical Systems

Deploy AI where safety matters.

**Features:**
- Watchdog timers trigger fail-safe if AI freezes
- ISO 26262 / IEC 61508 compliance hooks
- Audit logging for regulatory requirements

**Use cases:** Autonomous vehicles, industrial robots, medical devices.

### Fleet Management

Control hundreds of edge devices from one dashboard—now production-ready with real-time data.

**What you get:**
- **Real-time dashboard** showing all instances across regions
- Automatic device registration and health monitoring
- Fleet-wide metrics (CPU, memory, requests, errors)
- 30-second auto-refresh for live status updates
- Register devices automatically
- Push updates to entire fleet

**Dashboard features:**
- Instance health status (online/offline/maintenance)
- Performance metrics across all regions
- Capacity planning and utilization tracking
- Error rate monitoring with alerts

**Example:** Monitor 50 warehouse robots from a single dashboard, see which ones need attention instantly.

**Status:** Dashboard wired to production API as of Dec 2025. See [Fleet Management Guide](/docs/fleet-management).

---

## [1.7.0] - Coming Soon Core Reliability & Performance

**Planned:** Faster inference, persistent memory, better error handling.

### Guaranteed Response Times

Execute critical AI tasks with latency guarantees.

**Priority levels:**
- Critical: < 50ms response time
- High: < 200ms
- Normal: < 1s
- Low: < 5s

**Use case:** Safety-critical systems where timing matters.

### Automatic GPU Optimization

Runtime detects your hardware and configures itself for best performance.

**What happens:**
- Detects NVIDIA/AMD GPU or Apple Silicon
- Recommends optimal settings for your hardware
- Benchmarks performance automatically

**Result:** 2-5x faster inference without manual tuning.

### Persistent Agent Memory

Your AI remembers conversations across restarts.

**Features:**
- Semantic search over past interactions
- Long-term context retention
- Cache frequently-used responses

**Example:** AI remembers customer preferences from last week's conversation.

### Intelligent Error Recovery

AI automatically retries failed operations with smart backoff.

**What it handles:**
- Network timeouts (retry with exponential backoff)
- Rate limits (wait 60s, then retry)
- Fatal errors (fail immediately, don't waste time)

**Result:** More reliable AI in production.

### Vision & Audio Input ✅

Send images and audio to your AI—now production-ready.

**Supported formats:**
- **Images**: JPEG, PNG, GIF, WebP, BMP with real-time analysis
- **Audio**: WAV with duration, sample rate, and channel detection
- Base64 encoding for seamless API transmission

**What you get:**
- Image dimension detection and brightness analysis
- Audio metadata extraction (sample rate, channels, duration)
- Feature-gated compilation (include only what you need)
- Minimal overhead: ~38ms for images, ~13ms for audio

**Status:** Production-ready as of v1.6.0. See [Multi-Modal Guide](/docs/multimodal) for details.

---

## [1.6.0] - 2025-12-15

### Added

#### MCP Swarm Mode
- **Peer-to-peer context sharing** across multiple Runtime instances
- **mDNS auto-discovery** for zero-config swarm formation
- **AES-256-GCM encryption** for context synchronization
- **Real-time sync** via UDP multicast
- **Persistent storage** with encrypted key-value store

**Configuration:**
```json5
{
  mcp: {
    enabled: true,
    mdns: true,
    multicast: true,
    persist: true
  }
}
```

**Metrics:**
- `igris_mcp_peers_discovered` - Number of discovered peers
- `igris_mcp_contexts_synced_total` - Contexts synchronized
- `igris_mcp_sync_duration_seconds` - Sync latency

#### QLoRA On-Device Training
- **Automatic training** after N requests (configurable threshold)
- **Small LoRA adapters** (< 64 MB) for domain specialization
- **Encrypted storage** (AES-256-GCM) with device-specific keys
- **Auto-load trained adapters** on next inference
- **Training metrics** for monitoring progress

**Training Configuration:**
```json5
{
  lora_training: {
    enabled: true,
    trigger_threshold: 100,
    max_adapter_size_mb: 64,
    lora_rank: 8,
    lora_alpha: 16.0
  }
}
```

**Performance:**
- Raspberry Pi 4: ~15 minutes for 100 samples
- x86_64 (8 cores): ~5 minutes for 100 samples
- With GPU: ~2 minutes for 100 samples

### Improved

#### Multi-Agent Swarm Enhancements
- **Dynamic role assignment** based on task analysis
- **Consensus voting** with configurable candidates (default: 3)
- **Improved synthesis** for multi-agent responses
- **Better error handling** when agents fail

**New roles:**
- `researcher` - Information gathering
- `engineer` - Implementation planning
- `critic` - Quality assurance
- `synthesizer` - Response compilation

#### Reflection Agent Quality
- **Early stopping** when quality threshold met
- **Minimum improvement delta** to avoid infinite loops (0.05 default)
- **Verbose mode** for debugging reflection iterations
- **Better critique prompts** for specific feedback

**Performance improvement:**
- 30% fewer unnecessary iterations via early stopping
- 15% better quality scores with improved critique prompts

### Fixed
- Fixed race condition in MCP peer discovery
- Fixed memory leak in long-running reflection loops
- Fixed tool execution timeout not respected
- Fixed prompt cache corruption on concurrent requests
- Fixed LoRA adapter not loading after training

### Performance
- **50% faster model loading** via optimized memory mapping
- **2x faster token generation** with improved batch processing
- **30% lower memory usage** with prompt cache optimizations

---

## [1.5.0] - 2025-11-28

### Added

#### Planning Agents
- **Plan→Act→Observe→Reflect** chain-of-thought reasoning
- **Multi-step execution** with configurable max steps (default: 10)
- **Tool integration** for planning agents
- **Reflection support** for planning steps

**Configuration:**
```json5
{
  planning: {
    enabled: true,
    max_steps: 10,
    enable_reflection: true,
    enable_tools: false
  }
}
```

#### Multi-Agent Swarms
- **Parallel agent execution** with dynamic role assignment
- **Consensus mode** for multi-agent voting
- **Configurable swarm size** (default: 4)
- **Agent timeout** protection (default: 30s)

**Configuration:**
```json5
{
  swarm: {
    enabled: true,
    size: 4,
    max_concurrent: 4,
    agent_timeout_ms: 30000,
    dynamic_roles: true,
    consensus_candidates: 3
  }
}
```

#### Tool Use
- **HTTP tool** for external API calls
- **Shell tool** for command execution (sandboxed)
- **Filesystem tool** for file operations (sandboxed)
- **Whitelisting** for domains, commands, paths
- **Timeout enforcement** (default: 30s)
- **Concurrent execution limits** (default: 5)

**Configuration:**
```json5
{
  tools: {
    enable_http: true,
    enable_shell: false,
    enable_filesystem: false,
    allowed_http_domains: ["api.example.com"],
    allowed_shell_commands: ["ls", "cat"],
    allowed_filesystem_paths: ["/tmp"]
  }
}
```

### Improved
- **Better error messages** for model loading failures
- **Improved streaming** with reduced buffering
- **Better token counting** accuracy
- **Enhanced logging** with structured JSON output

### Fixed
- Fixed inference hanging on malformed prompts
- Fixed streaming not working with some models
- Fixed CPU usage not maxing out on multi-core systems
- Fixed context window overflow errors

---

## [1.4.0] - 2025-11-10

### Added

#### Reflection Agents
- **Self-improving AI** with Generate→Critique→Regenerate loop
- **Quality scoring** (0.0-1.0) for each iteration
- **Configurable threshold** for acceptance (default: 0.7)
- **Max iterations** limit (default: 3)

**Configuration:**
```json5
{
  reflection: {
    enabled: true,
    max_iterations: 3,
    quality_threshold: 0.7,
    temperature: 0.7
  }
}
```

**Performance:**
- Average 2.1 iterations to reach threshold
- 40% improvement in response quality
- Minimal latency increase (1.8x vs standard mode)

#### Speculative Execution
- **Race multiple providers** in parallel
- **First to respond wins** for lowest latency
- **Automatic fallback** if winner fails mid-stream
- **Cost tracking** for wasted requests

**Configuration:**
```json5
{
  routing: {
    speculative: {
      enabled: true,
      max_providers: 3,
      first_token_timeout_ms: 5000
    }
  }
}
```

### Improved
- **30% faster startup** via lazy model loading
- **Better GPU utilization** with layer offloading
- **Improved prompt caching** with KV cache reuse

### Fixed
- Fixed GPU layers not being used when configured
- Fixed model unloading not freeing memory
- Fixed concurrent requests causing context corruption

---

## [1.3.0] - 2025-10-25

### Added

#### Cloud Fallback Improvements
- **Thompson Sampling** for provider selection
- **Circuit breakers** for unhealthy providers
- **Rate limiting** per provider
- **Cost tracking** per request

**Configuration:**
```json5
{
  routing: {
    thompson_sampling: {
      enabled: true,
      exploration_rate: 0.1
    }
  }
}
```

#### Observability
- **Prometheus metrics** at `/metrics` endpoint
- **Structured JSON logging** via RUST_LOG
- **Health check** at `/v1/health`
- **Performance baselines** for common models

**Key metrics:**
- `igris_inference_requests_total`
- `igris_inference_duration_seconds`
- `igris_tokens_per_second`
- `igris_fallback_activations_total`

### Improved
- **Better error handling** for cloud provider failures
- **Faster fallback** (5s → 2s timeout)
- **Improved logging** with trace IDs

### Fixed
- Fixed fallback not triggering on timeout
- Fixed metrics not being exposed
- Fixed health check returning wrong status

---

## [1.2.0] - 2025-10-05

### Added

#### Multi-Model Support
- **Phi-3 Mini** (2.3 GB) - Recommended for most use cases
- **Mistral 7B** (4.1 GB) - Better quality, more RAM
- **Llama 3 8B** (4.7 GB) - Highest quality
- **Gemma 7B** (4.5 GB) - Alternative option

#### GPU Acceleration
- **CUDA support** for NVIDIA GPUs
- **Metal support** for Apple Silicon
- **Layer offloading** with configurable n_gpu_layers
- **5-10x faster** inference with GPU

**Configuration:**
```json5
{
  local_fallback: {
    n_gpu_layers: 32,  // -1 for all layers
    main_gpu: 0
  }
}
```

#### Performance Tuning
- **Prompt caching** for repeated queries
- **Batch processing** for better throughput
- **Configurable threads** to match CPU cores

### Improved
- **2x faster** model loading
- **Better memory management** for large models
- **Improved context handling** for long conversations

### Fixed
- Fixed out-of-memory errors with large context
- Fixed model loading failures on some systems
- Fixed incorrect token counts

---

## [1.1.0] - 2025-09-15

### Added

#### Local LLM Fallback (Core Feature)
- **Automatic fallback** from cloud to local on failure
- **GGUF model support** via llama.cpp integration
- **Configurable timeout** for cloud requests (default: 5s)
- **Works 100% offline** with `providers: []`

**Configuration:**
```json5
{
  local_fallback: {
    enabled: true,
    model_path: "models/phi-3-mini-4k-instruct-q4.gguf",
    context_size: 4096,
    threads: 4
  }
}
```

#### OpenAI-Compatible API
- **Drop-in replacement** for OpenAI SDK
- **Streaming support** via SSE
- **Chat completions** endpoint
- **Model listing** endpoint

**Endpoints:**
- `POST /v1/chat/completions` - Chat completions
- `GET /v1/models` - List available models
- `GET /v1/health` - Health check

#### Cloud Provider Support
- **OpenAI** (GPT-4, GPT-3.5)
- **Anthropic** (Claude 3 Sonnet, Claude 3 Opus)
- **Groq** (Llama models with fast inference)

### Features
- **Rust/Tokio**: Async runtime for performance
- **Axum HTTP server**: Fast, reliable HTTP handling
- **Redb embedded database**: No external database needed
- **JSON5 config**: Comments and trailing commas

### Fixed
- Initial release

---

## [1.0.0] - 2025-09-01

### Added
- Initial release of Igris Runtime
- Basic local inference with llama.cpp
- GGUF model loading
- Simple HTTP server
- Configuration file support

---

## Upcoming Releases

### [1.10.0] - Q1 2026 (Planned)

#### Features
- [ ] WebAssembly plugins for extensibility
- [ ] Enhanced agent types (code generation, research)
- [ ] Distributed training across MCP swarm
- [ ] Embeddings API support
- [ ] Vision model support (multimodal)

#### Improvements
- [ ] Faster MCP context sync
- [ ] Better QLoRA training performance
- [ ] Improved reflection quality scoring
- [ ] Enhanced tool sandboxing

### [1.11.0] - Q2 2026 (Planned)

#### Features
- [ ] Custom tool framework
- [ ] Advanced caching strategies
- [ ] Multi-language support (Python SDK, TypeScript SDK)
- [ ] Enhanced ROS2 integration (ROS2 Humble/Iron)
- [ ] Production-ready sensor integrations

---

## Migration Guides

### Migrating to v1.9.0 (Phase 3: Advanced Intelligence)

**No breaking changes.** All Phase 3 features are opt-in.

**To enable Federated Learning:**
```json5
{
  federated: {
    enabled: true,
    min_participants: 3,
    enable_differential_privacy: true
  }
}
```

**To enable Dynamic Model Management:**
```json5
{
  model_manager: {
    enabled: true,
    auto_select: true,
    max_loaded_models: 2
  }
}
```

**To enable Human-in-the-Loop:**
```json5
{
  hitl: {
    enabled: true,
    auto_approve_threshold: 0.9,
    timeout_secs: 300
  }
}
```

**To enable Simulation & Testing:**
```json5
{
  simulation: {
    enabled: true,
    chaos_enabled: true,
    failure_rate: 0.1
  }
}
```

### Migrating to v1.8.0 (Phase 2: Robotics & Edge)

**No breaking changes.** All Phase 2 features are opt-in.

**To enable ROS2 Integration:**
```json5
{
  ros2: {
    enabled: true,
    node_name: "igris_runtime_node",
    enable_nav2: true
  }
}
```

**To enable Sensors & Actuators:**
```json5
{
  sensors: {
    enabled: true,
    gpio_enabled: true,
    camera_enabled: true,
    allowed_gpio_pins: [17, 18, 27, 22]
  }
}
```

**To enable Safety & Certification:**
```json5
{
  safety: {
    enabled: true,
    watchdog_timeout_secs: 5,
    auto_failsafe: true,
    enable_audit_log: true
  }
}
```

**To enable Swarm Coordination:**
```json5
{
  swarm_coordination: {
    enabled: true,
    quorum_size: 3,
    enable_voting: true
  }
}
```

**To enable Fleet Management:**
```json5
{
  fleet: {
    enabled: true,
    overture_endpoint: "https://overture.example.com",
    agent_id: "robot-1"
  }
}
```

### Migrating to v1.6.0 (MCP Swarm + QLoRA)

**No breaking changes.** Both features are opt-in.

**To enable MCP Swarm:**
```json5
{
  mcp: {
    enabled: true,
    mdns: true,
    multicast: true
  }
}
```

**To enable QLoRA Training:**
```json5
{
  lora_training: {
    enabled: true,
    trigger_threshold: 100
  }
}
```

### Migrating to v1.5.0 (Planning + Swarm + Tools)

**No breaking changes.** All features are opt-in.

**To enable:**
```json5
{
  planning: { enabled: true },
  swarm: { enabled: true, size: 4 },
  tools: {
    enable_http: true,
    allowed_http_domains: ["api.example.com"]
  }
}
```

### Migrating to v1.4.0 (Reflection)

**No breaking changes.**

**To enable:**
```json5
{
  reflection: {
    enabled: true,
    max_iterations: 3,
    quality_threshold: 0.7
  }
}
```

---

## Release Notes

### Release Process

1. **Alpha** (internal testing)
2. **Beta** (early access)
3. **RC** (release candidate)
4. **GA** (general availability)

### Version Numbering

- **Major** (X.0.0): Breaking changes
- **Minor** (1.X.0): New features, backward compatible
- **Patch** (1.2.X): Bug fixes, security patches

### Support Policy

- **Current version**: Full support
- **Previous version**: Security patches only
- **Older versions**: End of life (EOL)

---

## Related Documentation

- [Quick Start](/docs/quickstart) - Get started in 5 minutes
- [Architecture](/docs/architecture) - System design and components
- [Deployment Guide](/docs/deployment) - Production deployment
- [Configuration](/docs/configuration) - Complete config reference
