# API Reference

Complete REST API reference for Igris Runtime. All endpoints are OpenAI-compatible with offline-first capabilities.

## Base URL

```
Local:       http://localhost:8080
Docker:      http://localhost:8080
Production:  http://your-server:8080
```

## Authentication

API requests can optionally require authentication via **API Key** (configurable in `config.json5`).

### API Key Authentication

```bash
curl http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json"
```

### Disable Authentication (Development)

```json5
// config.json5
{
  auth: {
    enabled: false  // No authentication required
  }
}
```

## Core Endpoints

<EndpointBlock method="GET" path="/v1/health">

### Health Check

Check API health and version.

**Response:**
```json
{
  "status": "ok",
  "service": "igris-runtime",
  "version": "1.6.0",
  "uptime_seconds": 86400,
  "local_llm_available": true,
  "model_loaded": true,
  "providers_healthy": 3
}
```

</EndpointBlock>

<EndpointBlock method="POST" path="/v1/infer">

### Inference (OpenAI-Compatible)

Perform inference with automatic provider selection.

**Also available at:** `/v1/chat/completions` (OpenAI-compatible alias)

**Request:**
```json
{
  "model": "gpt-4",
  "messages": [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Explain quantum computing"}
  ],
  "temperature": 0.7,
  "max_tokens": 1000,
  "stream": false,

  // Igris Runtime extensions
  "mode": "thompson",
  "local_fallback_enabled": true,
  "n_gpu_layers": 0,
  "prompt_cache": true
}
```

**Parameters:**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model` | string | required | Model identifier (e.g., "gpt-4", "phi3" for local) |
| `messages` | array | required | Array of message objects with role and content |
| `temperature` | float | 0.7 | Sampling temperature (0.0 - 2.0) |
| `max_tokens` | int | 512 | Maximum tokens to generate |
| `top_p` | float | 1.0 | Nucleus sampling parameter |
| `stream` | boolean | false | Enable Server-Sent Events streaming |
| `mode` | string | "thompson" | Routing mode: "thompson", "speculative", "council" |
| `local_fallback_enabled` | boolean | true | Enable automatic local LLM fallback |
| `n_gpu_layers` | int | 0 | GPU offload layers for local inference |
| `prompt_cache` | boolean | true | Enable prompt/KV caching |
| `provider` | string | auto | Specify provider or use "phi3" for local model |

**Response (Non-Streaming):**
```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1701234567,
  "model": "gpt-4",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Quantum computing is a revolutionary computing paradigm..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 150,
    "total_tokens": 175
  },
  "metadata": {
    "provider": "openai",
    "latency_ms": 234,
    "cost_usd": 0.0,
    "routing_decision": "thompson",
    "fallback_used": false,
    "local_model_available": true,
    "trace_id": "550e8400-e29b-41d4-a716-446655440000"
  }
}
```

**Response (Streaming):**
```
data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1701234567,"model":"gpt-4","choices":[{"index":0,"delta":{"role":"assistant","content":"Quantum"},"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1701234567,"model":"gpt-4","choices":[{"index":0,"delta":{"content":" computing"},"finish_reason":null}]}

data: {"id":"chatcmpl-abc123","object":"chat.completion.chunk","created":1701234567,"model":"gpt-4","choices":[{"index":0,"delta":{"content":" is"},"finish_reason":null}]}

...

data: [DONE]
```

</EndpointBlock>

<EndpointBlock method="POST" path="/v1/embeddings">

### Embeddings

Generate text embeddings (if supported by provider).

**Request:**
```bash
POST /v1/embeddings
```

**Request:**
```json
{
  "model": "text-embedding-ada-002",
  "input": "The quick brown fox jumps over the lazy dog",
  "encoding_format": "float"
}
```

**Response:**
```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "embedding": [0.0023, -0.0045, 0.0123, ...],
      "index": 0
    }
  ],
  "model": "text-embedding-ada-002",
  "usage": {
    "prompt_tokens": 10,
    "total_tokens": 10
  }
}
```

</EndpointBlock>

## Provider Management

<EndpointBlock method="GET" path="/v1/providers">

### List Providers

Get all available providers and their capabilities.

**Response:**
```json
{
  "providers": [
    {
      "name": "openai/gpt-4",
      "capabilities": {
        "models": ["gpt-4", "gpt-4-turbo", "gpt-4-32k"],
        "supports_streaming": true,
        "supports_vision": true,
        "supports_tools": true,
        "max_tokens": 8192,
        "max_context_window": 128000,
        "rate_limit_rpm": 10000,
        "cost_per_token": 0.00003
      },
      "health": "healthy",
      "last_health_check": "2025-11-30T12:00:00Z",
      "metrics": {
        "requests_total": 15234,
        "avg_latency_ms": 287,
        "error_rate": 0.012,
        "thompson_sampling_score": 0.87
      }
    },
    {
      "name": "anthropic/claude-3-5-sonnet",
      "capabilities": {
        "models": ["claude-3-5-sonnet-20241022"],
        "supports_streaming": true,
        "max_tokens": 4096,
        "max_context_window": 200000
      },
      "health": "healthy"
    }
  ]
}
```

</EndpointBlock>

<EndpointBlock method="GET" path="/v1/providers/{name}/stats">

### Provider Stats

Get detailed statistics for a specific provider.

**Response:**
```json
{
  "provider": "openai/gpt-4",
  "requests": 15234,
  "avg_latency_ms": 287,
  "p50_latency_ms": 245,
  "p95_latency_ms": 456,
  "p99_latency_ms": 678,
  "error_rate": 0.012,
  "total_cost_usd": 345.67,
  "avg_cost_per_request": 0.0227,
  "thompson_sampling_alpha": 12845.3,
  "thompson_sampling_beta": 183.7,
  "circuit_breaker_state": "closed",
  "last_error": null
}
```

</EndpointBlock>

<EndpointBlock method="POST" path="/v1/providers/custom">

### Register Custom Provider

Add a custom provider to the registry (Scale tier).

**Request:**
```json
{
  "name": "custom-llm",
  "base_url": "https://custom-llm.example.com/v1",
  "api_key": "custom-key",
  "capabilities": {
    "models": ["custom-model-1"],
    "supports_streaming": true,
    "max_tokens": 4096
  }
}
```

</EndpointBlock>

## Tenant Management (Multi-Tenancy)

<EndpointBlock method="POST" path="/v1/admin/tenants">

### Create Tenant
```

**Request:**
```json
{
  "name": "Acme Corp",
  "tier": "growth",
  "email": "admin@acme.com",
  "metadata": {
    "industry": "technology",
    "company_size": "50-200"
  }
}
```

**Response:**
```json
{
  "tenant_id": "tenant_abc123",
  "name": "Acme Corp",
  "tier": "growth",
  "api_keys": [
    {
      "key_id": "key_xyz789",
      "key_preview": "sk-...xyz789",
      "created_at": "2025-11-30T12:00:00Z"
    }
  ],
  "limits": {
    "max_requests_per_month": 2000000,
    "max_requests_per_second": 50,
    "max_providers": 10
  },
  "features": {
    "speculative_execution": true,
    "council_mode": true,
    "cognitive_advisor": true
  }
}
```

</EndpointBlock>

<EndpointBlock method="GET" path="/v1/tenants/{id}/usage">

### Get Tenant Usage

**Query Parameters:**
- `period`: "day", "week", "month" (default: "day")
- `start_date`: ISO 8601 date
- `end_date`: ISO 8601 date

**Response:**
```json
{
  "tenant_id": "tenant_abc123",
  "period": "month",
  "start_date": "2025-11-01T00:00:00Z",
  "end_date": "2025-11-30T23:59:59Z",
  "requests": {
    "total": 1456789,
    "successful": 1453211,
    "failed": 3578,
    "success_rate": 0.998
  },
  "cost": {
    "total_usd": 2345.67,
    "by_provider": {
      "openai": 1234.56,
      "anthropic": 987.65,
      "custom": 123.46
    },
    "budget_limit_usd": 5000.0,
    "budget_used_percent": 46.91
  },
  "latency": {
    "avg_ms": 287,
    "p50_ms": 234,
    "p95_ms": 456,
    "p99_ms": 678
  }
}
```

</EndpointBlock>

<EndpointBlock method="PATCH" path="/v1/admin/tenants/{id}/tier">

### Update Tenant Tier

**Request:**
```json
{
  "tier": "scale"
}
```

</EndpointBlock>

## Cost & Budget Management

<EndpointBlock method="GET" path="/v1/tenants/{id}/costs/breakdown">

### Get Cost Breakdown

**Response:**
```json
{
  "tenant_id": "tenant_abc123",
  "period": "month",
  "total_cost_usd": 2345.67,
  "breakdown": {
    "by_provider": {
      "openai": {
        "requests": 125600,
        "cost_usd": 1234.56,
        "avg_cost_per_request": 0.0098
      },
      "anthropic": {
        "requests": 98456,
        "cost_usd": 987.65,
        "avg_cost_per_request": 0.0100
      }
    },
    "by_model": {
      "gpt-4": 1100.23,
      "gpt-3.5-turbo": 134.33,
      "claude-3-sonnet": 987.65
    },
    "by_routing_policy": {
      "thompson-sampling": 1456.78,
      "speculative-execution": 678.90,
      "council-mode": 209.99
    }
  },
  "projections": {
    "end_of_month_usd": 2891.45,
    "overage_risk": "none"
  }
}
```

</EndpointBlock>

<EndpointBlock method="POST" path="/v1/tenants/{id}/budgets/alerts">

### Set Budget Alert

**Request:**
```json
{
  "threshold_percent": 90,
  "notification_channels": ["email", "webhook"],
  "webhook_url": "https://example.com/webhook"
}
```

</EndpointBlock>

## Routing & Optimization

<EndpointBlock method="POST" path="/v1/routing/preview">

### Preview Routing Decision

Preview which provider would be selected without executing.

**Request:**
```json
{
  "model": "gpt-4",
  "messages": [{"role": "user", "content": "Hello"}],
  "routing_policy": "thompson-sampling"
}
```

**Response:**
```json
{
  "selected_provider": "openai/gpt-4",
  "reason": "Highest Thompson Sampling score (0.87)",
  "confidence": 0.87,
  "alternatives": [
    {
      "provider": "anthropic/claude-3-sonnet",
      "score": 0.78,
      "estimated_cost_usd": 0.012,
      "estimated_latency_ms": 250
    }
  ],
  "estimated_cost_usd": 0.015,
  "estimated_latency_ms": 200,
  "semantic_classification": {
    "category": "conversational",
    "confidence": 0.85
  }
}
```

</EndpointBlock>

<EndpointBlock method="POST" path="/admin/optimizer">

### Update Optimizer Config

Hot-reload optimizer configuration (admin).

**Request:**
```json
{
  "mode": "rust",
  "sample_rate": 50,
  "slo_threshold_ms": 500
}
```

**Modes:**
- `go`: Use Go router only
- `rust`: Use Rust Thompson Sampling optimizer
- `shadow`: Run Rust in parallel for validation

</EndpointBlock>

<EndpointBlock method="GET" path="/admin/optimizer/status">

### Get Optimizer Status

**Response:**
```json
{
  "mode": "rust",
  "sample_rate": 50,
  "slo_threshold_ms": 500,
  "total_requests": 125678,
  "rust_requests": 62839,
  "go_fallbacks": 23,
  "slo_breaker_active": false,
  "avg_decision_time_us": 12.4
}
```

</EndpointBlock>

## Metrics & Monitoring

<EndpointBlock method="GET" path="/metrics">

### Prometheus Metrics

All metrics are exposed at the `/metrics` endpoint in Prometheus format.

**Key Metrics:**

**Request Metrics:**
```promql
# Total requests
http_requests_total{method="POST", path="/v1/infer", status="200"}

# Request duration
histogram_quantile(0.95, http_request_duration_seconds{method="POST"})

# Provider requests
inference_requests_total{provider="openai", model="gpt-4"}

# Provider latency
histogram_quantile(0.95, inference_latency_seconds{provider="openai"})
```

**Cost Metrics:**
```promql
# Total cost
cost_usd_total{provider="openai", component="inference"}

# Cost per request
rate(cost_usd_total[1h]) / rate(inference_requests_total[1h])

# Budget utilization
(tenant_cost_usd / tenant_budget_usd) * 100
```

**Routing Metrics:**
```promql
# Routing decisions
routing_decisions_total{policy="thompson-sampling"}

# Thompson Sampling scores
thompson_sampling_score{provider="openai"}

# Speculative execution savings
speculative_latency_saved_seconds{mode="latency"}

# Circuit breaker state
circuit_breaker_state{name="openai"}
```

</EndpointBlock>

<EndpointBlock method="GET" path="/v1/health/metrics">

### Health Metrics

**Response:**
```json
{
  "system": {
    "uptime_seconds": 86400,
    "cpu_usage_percent": 45.2,
    "memory_usage_mb": 2048,
    "goroutines": 1247
  },
  "providers": {
    "total": 5,
    "healthy": 5,
    "degraded": 0,
    "unhealthy": 0
  },
  "requests": {
    "total": 125678,
    "successful": 124891,
    "failed": 787,
    "success_rate": 0.994
  },
  "latency": {
    "p50_ms": 234,
    "p95_ms": 456,
    "p99_ms": 678
  }
}
```

</EndpointBlock>

## Execution Signing API (Hybrid Integration)

Runtime signs execution envelopes for cryptographic verification with Overture.

<EndpointBlock method="POST" path="/v1/execution/sign">

### Sign Execution Result

Sign an execution result for cryptographic verification. Used for Hybrid integration.

**Request:**
```json
{
  "decision_id": "dec_abc123",
  "decision_hash": "sha256:a8f3c2...",
  "result": {
    "provider": "anthropic",
    "model": "claude-3-opus",
    "latency_ms": 234,
    "tokens_used": 1247,
    "cost_usd": 0.0187,
    "success": true
  }
}
```

**Response:**
```json
{
  "execution_id": "exec_xyz789",
  "signed_envelope": {
    "execution_id": "exec_xyz789",
    "decision_id": "dec_abc123",
    "decision_hash": "sha256:a8f3c2...",
    "result": {...},
    "timestamp": "2026-01-26T12:00:00Z",
    "signature": "ed25519:b3e7f1..."
  }
}
```

</EndpointBlock>

<EndpointBlock method="POST" path="/v1/execution/verify-decision">

### Verify Routing Decision

Verify a signed routing decision from Overture before execution.

**Request:**
```json
{
  "signed_decision": {
    "decision_id": "dec_abc123",
    "timestamp": "2026-01-26T12:00:00Z",
    "selected_provider": "anthropic",
    "tenant_id": "tenant_xyz",
    "nonce": "nonce_123",
    "signature": "ed25519:a8f3c2..."
  },
  "tenant_public_key": "base64:MCowBQYDK2VwAyEA..."
}
```

**Response:**
```json
{
  "verified": true,
  "decision_id": "dec_abc123",
  "selected_provider": "anthropic",
  "timestamp_valid": true,
  "signature_valid": true,
  "nonce_valid": true
}
```

</EndpointBlock>

<EndpointBlock method="GET" path="/v1/crypto/keypair">

### Get Runtime Keypair Info

Get information about the Runtime's signing keypair.

**Response:**
```json
{
  "public_key": "base64:MCowBQYDK2VwAyEA...",
  "algorithm": "Ed25519",
  "key_id": "runtime_key_abc123",
  "created_at": "2026-01-01T00:00:00Z"
}
```

</EndpointBlock>

<EndpointBlock method="POST" path="/v1/crypto/keypair/rotate">

### Rotate Runtime Keypair

Generate a new signing keypair for the Runtime.

**Request:**
```json
{
  "confirm": true
}
```

**Response:**
```json
{
  "new_key_id": "runtime_key_xyz789",
  "new_public_key": "base64:MCowBQYDK2VwAyEA...",
  "old_key_id": "runtime_key_abc123",
  "rotated_at": "2026-01-26T12:00:00Z"
}
```

</EndpointBlock>

## Fleet Management API

Manage Runtime fleet registration and telemetry with Overture.

<EndpointBlock method="POST" path="/v1/fleet/register">

### Register with Fleet

Register this Runtime instance with the Overture fleet manager.

**Request:**
```json
{
  "fleet_id": "fleet_abc123",
  "agent_name": "runtime-prod-01",
  "capabilities": {
    "local_llm": true,
    "gpu_available": true,
    "models": ["phi3", "llama3"]
  },
  "overture_endpoint": "https://api.igrisinertial.com"
}
```

**Response:**
```json
{
  "registered": true,
  "agent_id": "agent_xyz789",
  "fleet_id": "fleet_abc123",
  "heartbeat_interval_seconds": 30,
  "config_version": 1
}
```

</EndpointBlock>

<EndpointBlock method="POST" path="/v1/fleet/heartbeat">

### Send Heartbeat

Send a health heartbeat to the fleet manager.

**Response:**
```json
{
  "acknowledged": true,
  "config_update_available": false,
  "next_heartbeat_seconds": 30
}
```

</EndpointBlock>

<EndpointBlock method="GET" path="/v1/fleet/config">

### Get Fleet Configuration

Get the current fleet configuration for this Runtime.

**Response:**
```json
{
  "config_version": 1,
  "routing_policy": "thompson-sampling",
  "providers_enabled": ["openai", "anthropic", "local"],
  "local_model_config": {
    "model": "phi3",
    "n_gpu_layers": 0,
    "context_size": 4096
  },
  "fallback_enabled": true
}
```

</EndpointBlock>

## Admin Endpoints

<EndpointBlock method="POST" path="/admin/routing/reset">

### Reset Thompson Sampling

Reset all Thompson Sampling state (admin).

</EndpointBlock>

<EndpointBlock method="POST" path="/admin/config/reload">

### Reload Configuration

Hot-reload tier configuration without restart.

</EndpointBlock>

<EndpointBlock method="POST" path="/admin/circuit-breaker/{provider}/reset">

### Circuit Breaker Control

Manually control circuit breaker state.

**Available actions:**
- `/admin/circuit-breaker/{provider}/reset`
- `/admin/circuit-breaker/{provider}/open`
- `/admin/circuit-breaker/{provider}/close`

</EndpointBlock>

## Error Responses

All errors follow a consistent format:

```json
{
  "error": {
    "type": "authentication_error",
    "code": "invalid_api_key",
    "message": "The API key provided is invalid",
    "request_id": "req_abc123",
    "documentation_url": "https://docs.igrisinertial.com/errors/invalid_api_key"
  }
}
```

**HTTP Status Codes:**

| Code | Meaning | Common Causes |
|------|---------|---------------|
| 200 | Success | Request completed successfully |
| 400 | Bad Request | Invalid parameters or malformed JSON |
| 401 | Unauthorized | Missing or invalid API key |
| 402 | Payment Required | Budget limit exceeded |
| 403 | Forbidden | Insufficient permissions or tier limits |
| 404 | Not Found | Resource does not exist |
| 429 | Too Many Requests | Rate limit exceeded |
| 500 | Internal Server Error | Server-side error |
| 502 | Bad Gateway | Provider API error |
| 503 | Service Unavailable | All providers unhealthy |

**Error Types:**

- `authentication_error`: API key invalid or expired
- `authorization_error`: Insufficient permissions
- `rate_limit_error`: Too many requests
- `budget_limit_error`: Monthly budget exceeded
- `validation_error`: Invalid request parameters
- `provider_error`: Provider API returned error
- `timeout_error`: Request timed out
- `circuit_breaker_error`: Provider circuit breaker open

## Rate Limits

Rate limits are tier-based and enforced per tenant:

| Tier | Requests/Second | Requests/Minute | Requests/Month |
|------|----------------|-----------------|----------------|
| Trial | 10 | 300 | 50,000 |
| Developer | 10 | 300 | 500,000 |
| Growth | 50 | 1,500 | 2,000,000 |
| Scale | 1,000 | 60,000 | Unlimited |

**Rate Limit Headers:**
```
X-RateLimit-Limit: 50
X-RateLimit-Remaining: 42
X-RateLimit-Reset: 1701234567
```

## Versioning

The API uses URL versioning. Current version is **v1**.

```
https://api.igrisinertial.com/v1/infer
```

When breaking changes are introduced, a new version (v2) will be released alongside v1.

## WebSocket API (Real-time Updates)

Connect to WebSocket for real-time streaming and updates.

```javascript
const ws = new WebSocket('wss://api.igrisinertial.com/v1/stream');

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log('Received:', data);
};

// Subscribe to job updates
ws.send(JSON.stringify({
  type: 'subscribe',
  channel: 'job',
  job_id: 'job_abc123'
}));
```

## SDK Support

Use standard OpenAI SDKs with base URL override:
- **Python**: `pip install openai` (set `base_url="http://localhost:8080/v1"`)
- **JavaScript/TypeScript**: `npm install openai` (set `baseURL`)
- **Go**: `go get github.com/sashabaranov/go-openai` (set `BaseURL`)
- **Rust**: `cargo add async-openai` (set base URL in config)

Or use HTTP directly - Igris Runtime is 100% OpenAI-compatible.

See [SDK Usage](/docs/api-reference/sdks) for detailed examples.

## Related Documentation

- [Provider Keys](/docs/providers-keys) - Configure provider API keys
- [Routing Policies](/docs/routing-policies) - Understand routing strategies
- [Multi-Tenancy](/docs/multi-tenancy) - Tenant isolation and management
- [Observability](/docs/observability) - Monitoring and tracing
- [Pricing](/docs/pricing) - Tier limits and pricing
