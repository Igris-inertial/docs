# API Reference

**TL;DR:** Igris Overture is an OpenAI-compatible inference gateway with superpowers. One URL change, infinite providers.

```bash
curl https://api.igrisinertial.com/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Philosophy

Igris Overture provides **100% OpenAI-compatible** endpoints with enterprise-grade enhancements:

- **Drop-in replacement**: Change your base URL from `api.openai.com` to `api.igrisinertial.com`
- **Multi-provider routing**: Automatic failover across OpenAI, Anthropic, and custom providers
- **Thompson Sampling**: ML-powered routing that learns which provider works best
- **Speculative Execution**: Race providers for ultra-low latency
- **Council Mode**: Get consensus from multiple models
- **Cost optimization**: Real-time budget tracking and automatic fallback

## Base URLs

```
Production:  https://api.igrisinertial.com/v1
Staging:     https://api-staging.igrisinertial.com/v1
Local:       http://localhost:8080/v1
```

## Quick Start

1. **Get your API key** from the [dashboard](https://console.igrisinertial.com)
2. **Change your base URL**:
   ```diff
   - https://api.openai.com/v1/chat/completions
   + https://api.igrisinertial.com/v1/chat/completions
   ```
3. **You're live** — your existing OpenAI SDK code works unchanged

## Igris Overture-Specific Extensions

While we're 100% OpenAI-compatible, we add optional superpowers:

```json
{
  "model": "gpt-4",
  "messages": [...],

  // Standard OpenAI params
  "temperature": 0.7,
  "max_tokens": 1000,

  // Igris Overture features (optional)
  "routing_policy": "thompson-sampling",
  "speculative_mode": "latency",
  "council_mode": true,
  "optimization": "balanced"
}
```

<details>
<summary>Show all Igris Overture extensions</summary>

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `routing_policy` | string | `"thompson-sampling"` | Routing strategy: `"thompson-sampling"`, `"cost-optimized"`, `"latency-optimized"` |
| `speculative_mode` | string | `null` | Race providers: `"latency"`, `"balanced"`, `"quality"`, `"cost"` |
| `council_mode` | boolean | `false` | Get consensus from multiple models (Growth+ tier) |
| `provider` | string | auto | Force specific provider: `"openai"`, `"anthropic"`, etc. |
| `optimization` | string | `"balanced"` | Optimization goal: `"latency"`, `"cost"`, `"quality"`, `"balanced"` |
| `enable_semantic_routing` | boolean | `true` | Use semantic classification for routing |

</details>

## Response Format

Standard OpenAI format with additional metadata:

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1701234567,
  "model": "gpt-4",
  "choices": [...],
  "usage": {
    "prompt_tokens": 25,
    "completion_tokens": 150,
    "total_tokens": 175
  },

  // Igris Overture metadata
  "metadata": {
    "provider": "openai",
    "latency_ms": 234,
    "cost_usd": 0.00525,
    "routing_decision": "thompson-sampling",
    "thompson_sampling_score": 0.87,
    "trace_id": "550e8400-e29b-41d4-a716-446655440000"
  }
}
```

## Next Steps

- [Quick Start](/docs/api-reference/quick-start) — First working curl in 60 seconds
- [Authentication](/docs/api-reference/authentication) — Get your API key
- [Chat Completions](/docs/api-reference/endpoints/chat-completions) — Main inference endpoint
- [SDKs](/docs/api-reference/sdks) — TypeScript, Python, Go clients
