# Provider Health Checks

**Continuous validation. Instant failover. Zero manual intervention.**

Provider Health Checks continuously monitor endpoint availability and performance, automatically routing traffic away from degraded providers before they impact your users.

---

## Quick Example

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.igrisinertial.com/v1",
    api_key="your-key"
)

# Health checks run automatically in the background
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)

# If OpenAI is degraded, request automatically routes to healthy alternative
print(response.metadata)
# {
#   "provider": "anthropic",
#   "routing_reason": "primary_unhealthy",
#   "health_status": {
#     "openai": "degraded",
#     "anthropic": "healthy"
#   }
# }
```

---

## What You Get

### Continuous Monitoring

Real-time health validation:

**Active Probing**: Endpoint checks every 30 seconds

**Passive Monitoring**: Real request success rates tracked

**Latency Tracking**: Response time degradation detected instantly

**Multi-Region Checks**: Validates availability across geographic regions

### Automatic Failover

Instant traffic redirection:

**Sub-Second Detection**: Health changes identified in under 500ms

**Graceful Degradation**: Traffic shifts smoothly to healthy providers

**Automatic Recovery**: Re-enables providers once health restored

**Circuit Breaker**: Prevents cascading failures

---

## How It Works

<details>
<summary><strong>1. Health Check Types</strong></summary>

Multiple validation methods ensure accuracy:

**Active Health Checks:**
- Lightweight test requests every 30 seconds
- Validates endpoint reachability
- Measures baseline latency
- Confirms authentication still valid

**Passive Health Checks:**
- Monitors actual user request success rates
- Tracks error patterns in real traffic
- Detects partial degradation
- More accurate than synthetic tests alone

**Combined Scoring:**
```
Health Score = (Active Check × 0.3) + (Passive Success × 0.7)
```

</details>

<details>
<summary><strong>2. Health States</strong></summary>

Providers can be in four states:

**Healthy (100%)**: All checks passing, normal routing

**Degraded (50-99%)**: Some failures detected
- Reduced traffic allocation
- Increased monitoring frequency
- Warning alerts sent

**Unhealthy (0-49%)**: Significant failures
- Traffic redirected to alternatives
- No new requests routed
- Critical alerts sent

**Unknown**: Initial state or timeout
- Conservative routing (50% allocation)
- Aggressive health probing

</details>

<details>
<summary><strong>3. Failover Process</strong></summary>

When degradation is detected:

**Phase 1: Detection (0-500ms)**
- Health check failure detected
- Confirm with second immediate check
- Mark provider as degraded

**Phase 2: Traffic Shift (0.5-2s)**
- Reduce traffic to degraded provider by 80%
- Increase traffic to healthy alternatives
- Monitor for improvement

**Phase 3: Full Failover (2-5s)**
- If still degraded, remove from rotation
- Redistribute traffic across healthy providers
- Continue health checks every 10s

**Phase 4: Recovery (60s+)**
- When health restored, gradual re-introduction
- Start with 10% traffic, increase to 100% over 5 minutes
- Monitor closely during recovery

</details>

<details>
<summary><strong>4. Circuit Breaker</strong></summary>

Prevents request waste on failing providers:

**Open Circuit**: Provider marked unhealthy
- Zero traffic routed
- Health checks continue in background
- Fast-fail for pending requests

**Half-Open Circuit**: Provider recovering
- Limited traffic (10%) for testing
- Strict monitoring of success rate
- Ready to re-open or close again

**Closed Circuit**: Provider fully healthy
- Normal traffic flow
- Standard monitoring resumes

</details>

---

## Configuration

### Health Check Settings

Customize monitoring behavior:

```yaml
health_check_config:
  # Active health checks
  active_interval: 30s        # How often to probe
  active_timeout: 5s          # Probe timeout
  active_endpoint: "/v1/health"  # Health endpoint

  # Passive monitoring
  passive_window: 5m          # Success rate window
  passive_threshold: 0.95     # Min 95% success

  # Thresholds
  healthy_threshold: 0.95     # Mark healthy at 95%+
  degraded_threshold: 0.80    # Mark degraded below 80%
  unhealthy_threshold: 0.50   # Mark unhealthy below 50%
```

### Failover Behavior

Control how traffic shifts:

```yaml
failover_config:
  # Detection speed
  confirmation_checks: 2      # Require 2 failures before failover
  check_interval_degraded: 10s  # Faster checks when degraded

  # Traffic shifting
  degraded_traffic_pct: 20    # 20% traffic when degraded
  recovery_ramp_duration: 5m  # 5min gradual recovery

  # Circuit breaker
  circuit_open_duration: 60s  # How long circuit stays open
  circuit_half_open_requests: 10  # Test requests in half-open
```

### Alert Configuration

Get notified of health changes:

```yaml
alerts:
  # Notify on health changes
  degraded_alert: true        # Alert when degraded
  unhealthy_alert: true       # Alert when unhealthy
  recovery_alert: true        # Alert when recovered

  # Channels
  slack_webhook: "https://hooks.slack.com/..."
  email_recipients: ["ops@company.com"]
  pagerduty_key: "..."

  # Thresholds
  alert_after_failures: 3     # Alert after 3 consecutive failures
  alert_cooldown: 5m          # Min time between alerts
```

---

## Health Check Endpoints

### Get Provider Health Status

```bash
GET /v1/health/providers
```

**Response:**
```json
{
  "timestamp": "2025-12-09T10:15:30Z",
  "providers": {
    "openai": {
      "status": "healthy",
      "score": 0.98,
      "last_check": "2025-12-09T10:15:25Z",
      "latency_ms": 145,
      "success_rate_24h": 0.995,
      "circuit_state": "closed"
    },
    "anthropic": {
      "status": "healthy",
      "score": 0.96,
      "last_check": "2025-12-09T10:15:26Z",
      "latency_ms": 167,
      "success_rate_24h": 0.992,
      "circuit_state": "closed"
    },
    "google": {
      "status": "degraded",
      "score": 0.82,
      "last_check": "2025-12-09T10:15:24Z",
      "latency_ms": 523,
      "success_rate_24h": 0.875,
      "circuit_state": "half_open",
      "degradation_reason": "high_latency"
    }
  }
}
```

### Get Health History

```bash
GET /v1/health/history?provider=openai&period=24h
```

**Response:**
```json
{
  "provider": "openai",
  "period": "24h",
  "events": [
    {
      "timestamp": "2025-12-08T15:30:00Z",
      "status": "degraded",
      "score": 0.75,
      "reason": "error_rate_spike",
      "duration": "15m"
    },
    {
      "timestamp": "2025-12-08T15:45:00Z",
      "status": "healthy",
      "score": 0.98,
      "reason": "recovered"
    }
  ],
  "uptime_pct": 99.2
}
```

### Force Health Check

```bash
POST /v1/health/providers/openai/check
```

Immediately triggers health check for specified provider.

---

## Observability

### Dashboard View

Real-time health monitoring:

```
Provider Health Status
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
OpenAI GPT-4:       Healthy (98%)
  Last Check: 5s ago
  Latency: 145ms
  24h Uptime: 99.8%

Anthropic Claude:   Healthy (96%)
  Last Check: 4s ago
  Latency: 167ms
  24h Uptime: 99.7%

Google Gemini:      Degraded (82%)
  Last Check: 6s ago
  Latency: 523ms 
  24h Uptime: 96.2%
  Circuit: Half-Open

Last 24h: 3 degradation events, avg recovery: 8 minutes
```

### Prometheus Metrics

Track health check performance:

```
provider_health_score{provider="openai"}
provider_health_checks_total{provider="openai",result="success"}
provider_circuit_state{provider="openai",state="closed"}
provider_failover_events_total{from="openai",to="anthropic"}
provider_latency_ms{provider="openai",check_type="active"}
provider_uptime_pct{provider="openai",period="24h"}
```

### Alert Events

Health change notifications:

```json
{
  "timestamp": "2025-12-09T10:15:30Z",
  "event_type": "provider_degraded",
  "provider": "openai",
  "previous_state": "healthy",
  "current_state": "degraded",
  "health_score": 0.82,
  "reason": "latency_spike",
  "details": {
    "avg_latency_ms": 523,
    "p99_latency_ms": 1247,
    "error_rate": 0.08
  },
  "action_taken": "reduce_traffic_to_20_pct",
  "impact": {
    "requests_redirected": 1247,
    "traffic_shift_pct": 80
  }
}
```

---

## Benefits

### Eliminate Manual Monitoring

Automatic detection and response:

**24/7 Monitoring**: Never miss provider issues, even at 3am

**Instant Response**: Failover happens in under 1 second

**No Pager Duty**: System handles degradation automatically

**Clear Visibility**: Dashboards show exactly what's happening

### Improve Reliability

Real-world impact:

**99.9%+ Request Success**: Automatic failover prevents failures

**50% Faster Recovery**: No waiting for manual intervention

**Zero User Impact**: Degradation handled before users affected

**Reduced Alerts**: Only critical issues require attention

### Cost Savings

Efficiency improvements:

**No Wasted Requests**: Circuit breaker prevents failed calls

**Optimal Resource Use**: Traffic flows only to healthy providers

**Reduced On-Call**: Fewer emergency escalations

**Better SLA Compliance**: Automatic maintenance of uptime goals

---

## Use Cases

### Multi-Provider Redundancy

**Scenario**: Use OpenAI primary, Anthropic backup

**Configuration**:
- OpenAI: 80% traffic when healthy
- Anthropic: 20% traffic, scales to 100% on failover
- Automatic recovery when OpenAI restored

**Result**: Seamless failover during OpenAI outages

### Global Deployment

**Scenario**: Serve requests from multiple regions

**Configuration**:
- Regional health checks in US, EU, APAC
- Failover to nearest healthy region
- Latency-based routing when all healthy

**Result**: Consistent performance despite regional issues

### SLA-Critical Production

**Scenario**: 99.99% uptime requirement

**Configuration**:
- Aggressive health checking (every 10s)
- Instant failover on first failure
- Multiple backup providers configured

**Result**: SLA maintained even during provider incidents

---

## Best Practices

### Configure Multiple Providers

Don't rely on single provider:

```yaml
providers:
  primary: "openai"
  fallback: ["anthropic", "google", "cohere"]
  min_healthy: 2  # Require at least 2 healthy providers
```

### Set Appropriate Thresholds

Balance sensitivity and stability:

- **Too Aggressive**: Frequent unnecessary failovers
- **Too Relaxed**: Delayed response to real issues
- **Recommended**: 3 consecutive failures before failover

### Monitor Health Trends

Watch for patterns:

- Time-of-day degradation (peak hours)
- Geographic patterns (regional issues)
- Correlation with deployments
- Seasonal variations

### Test Failover Regularly

Validate your configuration:

1. Manually trigger health check failure
2. Verify traffic shifts correctly
3. Confirm alerts fire as expected
4. Test recovery process

---

## Availability

Provider Health Checks are included in:

- **Developer Tier**: Basic health checks (every 60s)
- **Growth Tier**: Standard health checks (every 30s) with failover
- **Scale Tier**: Advanced health checks (every 10s) with multi-region monitoring

---

## Related Features

- [SLO Enforcer](/docs/core-features/slo-enforcer) - Automatic remediation
- [Adaptive Optimization](/docs/core-features/adaptive-optimization) - Performance tuning
- [EscapeVector](/docs/core-features/escape-vector) - Offline failover

---

**Questions?** Email support@igrisinertial.com or join our [Discord](https://discord.gg/igris-overture)
