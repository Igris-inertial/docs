# Speculative Execution

**60% faster time-to-first-token. Same cost as single provider.**

Race multiple AI providers in parallel and get the fastest high-quality response. Unlike simple racing, we use Bayesian optimization to pick the best answer — not just the quickest.

---

## Quick Example

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.igrisinertial.com/v1",
    api_key="your-key"
)

# Enable speculative mode
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Explain quantum computing"}],
    extra_body={"speculative_mode": "latency"}
)

print(response.metadata)
# {
#   "speculative": true,
#   "providers_raced": ["openai", "anthropic", "google"],
#   "winner": "anthropic",
#   "time_to_first_token_ms": 180,
#   "improvement_vs_single": "62%"
# }
```

---

## What You Get

### 60% Faster Responses

Race 2-3 providers simultaneously:

**Parallel Execution**: All providers queried at once

**Intelligent Winner**: Best quality response wins, not just fastest

**Auto-Cancellation**: Slow providers cancelled to save cost

**Smart Selection**: Only races when providers are closely matched

### Bayesian Winner Selection

Unlike naive racing (fastest wins), we consider:

- **Latency**: Time to first token
- **Quality**: Response completeness and accuracy
- **Cost**: Provider pricing
- **Historical Performance**: Thompson Sampling scores

The **best** response wins, not necessarily the fastest.

---

## How It Works

<details>
<summary><strong>1. Race Decision (Automatic)</strong></summary>

The system decides when to race:

**Race when:**
- Multiple providers have similar Thompson Sampling scores (`<15%` difference)
- Request is latency-sensitive (streaming, interactive)
- Cost budget allows parallel execution

**Don't race when:**
- One provider is clearly best (>15% confidence gap)
- Cost optimization mode enabled
- Only one provider available

</details>

<details>
<summary><strong>2. Parallel Provider Queries</strong></summary>

Top 2-3 providers queried simultaneously:

```
t=0ms:  Start OpenAI, Anthropic, Google
t=180ms: Anthropic returns first token ← Winner
t=240ms: OpenAI returns (cancelled)
t=350ms: Google returns (cancelled)
```

**Smart Cancellation:**
- When winner is chosen, others are cancelled
- Partial results saved to reduce waste
- Cost tracked and reported

</details>

<details>
<summary><strong>3. Bayesian Scoring</strong></summary>

Each response is scored:

```
Score = (Latency_Score * 0.4) +
        (Quality_Score * 0.4) +
        (Cost_Score * 0.2)
```

**Latency Score**: Time to first token (0-1 normalized)

**Quality Score**: Response completeness and accuracy

**Cost Score**: Provider pricing (cheaper = higher score)

Highest composite score wins.

</details>

<details>
<summary><strong>4. Learning & Optimization</strong></summary>

Race results update Thompson Sampling:

- Winner's parameters improved
- Losers' parameters adjusted
- Future races optimized based on learnings

This means **races get smarter over time**.

</details>

---

## Modes

### Latency Mode (Default)

Optimize for fastest response:

```bash
curl -X POST https://api.igrisinertial.com/v1/infer \
  -d '{
    "model": "gpt-4",
    "messages": [...],
    "speculative_mode": "latency"
  }'
```

**Best for:**
- Interactive chat applications
- Real-time assistants
- Streaming responses

### Quality Mode

Prioritize response quality:

```bash
curl -X POST https://api.igrisinertial.com/v1/infer \
  -d '{
    "model": "gpt-4",
    "messages": [...],
    "speculative_mode": "quality"
  }'
```

**Best for:**
- Code generation
- Complex reasoning
- Critical decision-making

### Balanced Mode

Equal weight on latency, quality, cost:

```bash
curl -X POST https://api.igrisinertial.com/v1/infer \
  -d '{
    "model": "gpt-4",
    "messages": [...],
    "speculative_mode": "balanced"
  }'
```

**Best for:**
- General-purpose applications
- Unknown workloads
- Cost-conscious deployments

---

## Cost Optimization

### Intelligent Cancellation

As soon as a winner is chosen:
- Other providers are cancelled immediately
- Only pay for tokens actually used
- Waste typically `<10%` of single-provider cost

### Cost Tracking

Every response includes waste metrics:

```json
{
  "metadata": {
    "speculative": {
      "providers_raced": 3,
      "winner": "anthropic",
      "cost_usd": 0.00234,
      "wasted_cost_usd": 0.00021,
      "waste_ratio": 0.09,
      "net_speedup": "62%"
    }
  }
}
```

### Auto-Disable on High Waste

If waste ratio exceeds 30% for 100+ requests:
- Speculative mode automatically pauses
- Alert sent to dashboard
- Manual re-enable required

This prevents runaway costs.

---

## Performance

### Real-World Results

| Metric | Single Provider | Speculative (2 providers) | Speculative (3 providers) |
|--------|----------------|---------------------------|---------------------------|
| **Time to First Token** | 450ms | 180ms (-60%) | 165ms (-63%) |
| **P95 Latency** | 890ms | 380ms (-57%) | 340ms (-62%) |
| **Cost Multiplier** | 1.0x | 1.08x (+8%) | 1.12x (+12%) |
| **Quality Score** | 8.4/10 | 8.7/10 (+4%) | 8.9/10 (+6%) |

### When It Shines

**Best improvements:**
- Streaming responses: 60-70% faster
- Interactive chat: 55-65% faster
- Code generation: 50-60% faster

**Modest improvements:**
- Batch processing: 20-30% faster
- Large responses: 15-25% faster

---

## Configuration

### Enable via Request

```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[...],
    extra_body={
        "speculative_mode": "latency",
        "max_providers": 3,
        "max_waste_ratio": 0.25
    }
)
```

### Enable via Dashboard

```
Settings → Features → Speculative Execution → Enable
```

**Options:**
- Mode: latency, quality, balanced
- Max Providers: 2-5 (default: 3)
- Max Waste Ratio: 0.1-0.5 (default: 0.3)

---

## Pricing

- **Developer Tier**: Not available
- **Growth Tier**: Included (up to 10k speculative requests/month)
- **Scale Tier**: Unlimited

[View Pricing →](/docs/pricing-tiers)

---

## FAQ

<details>
<summary><strong>How much does it cost?</strong></summary>

Typically 8-12% more than single-provider due to cancelled requests. Auto-disables if waste exceeds 30%.

</details>

<details>
<summary><strong>Does it always race?</strong></summary>

No. Only races when:
1. Multiple providers have similar scores (`<15%` difference)
2. Speculative mode is enabled
3. Cost budget allows it

</details>

<details>
<summary><strong>Can I control which providers race?</strong></summary>

Yes. Set provider preferences in dashboard or via `preferred_providers` parameter.

</details>

<details>
<summary><strong>What about streaming?</strong></summary>

Speculative mode works with streaming! Returns tokens from winner as they arrive.

</details>

<details>
<summary><strong>How is this different from simple racing?</strong></summary>

Simple racing returns fastest response. Speculative Execution uses Bayesian scoring to pick the **best** response considering latency, quality, and cost.

</details>

---

## Best Practices

### When to Use

**Use speculative mode for:**
- Interactive chat applications
- Real-time assistants
- Time-sensitive queries
- High-value requests

**Don't use for:**
- Batch processing (cost inefficient)
- Budget-constrained workloads
- Single-provider deployments

### Monitoring

Track these metrics:
- **Waste Ratio**: Should stay `<20%`
- **Speedup**: Should see 50%+ improvement
- **Quality**: Winner selection accuracy

If waste ratio climbs above 25%, investigate or disable.

---

## Related Features

- [Thompson Sampling](/docs/routing-policies) - Intelligent provider selection
- [EscapeVector](/docs/core-features/escape-vector) - Offline failover
- [Observability](/docs/observability) - Performance monitoring

---

## Get Started

Enable speculative mode in your next request:

```python
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}],
    extra_body={"speculative_mode": "latency"}
)
```

See 60% faster responses immediately.

**Questions?** Email support@igrisinertial.com
